Lecture-1
---
Welcome to this course on the practical
aspects of deep learning. Perhaps now you've learned how
to implement a neural network. In this week, you'll learn
the practical aspects of how to make your neural network work well. Ranging from things like hyperparameter
tuning to how to set up your data, to how to make sure your optimization
algorithm runs quickly so that you get your learning algorithm
to learn in a reasonable amount of time. In this first week, we'll first talk about
how the cellular machine learning problem, then we'll talk about randomization, then we'll talk about some tricks for making sure your neural network
implementation is correct. With that, let's get started. Making good choices in how you set
up your training, development, and test sets can make a huge
difference in helping you quickly find a good high-performance neural network. When training a neural network, you
have to make a lot of decisions, such as how many layers will
your neural network have? And, how many hidden units do
you want each layer to have? And, what's the learning rate? And, what are the activation functions you
want to use for the different layers? When you're starting on a new application, it's almost impossible to correctly
guess the right values for all of these, and for other hyperparameter
choices, on your first attempt. So, in practice, applied machine learning
is a highly iterative process, in which you often start with an idea, such as you want to build a neural
network of a certain number of layers, a certain number of hidden units,
maybe on certain data sets, and so on. And then you just have to code it up and
try it, by running your code. You run an experiment and
you get back a result that tells you how well this particular network, or
this particular configuration works. And based on the outcome, you might then refine your ideas and
change your choices and maybe keep iterating, in order to try to
find a better and a better, neural network. Today, deep learning has found
great success in a lot of areas ranging from natural language
processing, to computer vision, to speech recognition, to a lot of
applications on also structured data. And structured data includes everything
from advertisements to web search, which isn't just Internet search engines.
It's also, for example, shopping websites. Already any website that wants to deliver great search results when
you enter terms into a search bar. To computer security, to logistics,
such as figuring out where to send drivers to pick up and
drop off things...to many more. So what I'm seeing is that sometimes
a researcher with a lot of experience in NLP might enter...you know, might try to do
something in computer vision. Or maybe a researcher with a lot of
experience in speech recognition might, you know, jump in and
try to do something on advertising. Or someone from security might want to
jump in and do something on logistics. And what I've seen is that
intuitions from one domain or from one application area often do not
transfer to other application areas. And the best choices may depend
on the amount of data you have, the number of input features you have
through your computer configuration and whether you're training on GPUs or CPUs. And if so, exactly what configuration of
GPUs and CPUs...and many other things. So, for a lot of applications, I
think it's almost impossible. Even very experienced deep learning people
find it almost impossible to correctly guess the best choice of
hyperparameters the very first time. And so today,
applied deep learning is a very iterative process where you just have to
go around this cycle many times to hopefully find a good choice
of network for your application. So one of the things that determine
how quickly you can make progress is how efficiently you can
go around this cycle. And setting up your data sets well, in
terms of your train, development and test sets can make you much
more efficient at that. So if this is your training data,
let's draw that as a big box. Then traditionally, you might
take all the data you have and carve off some portion of
it to be your training set, some portion of it to be your
hold-out cross validation set, and this is sometimes also
called the development set. And for brevity, I'm just going to
call this the dev set, but all of these terms mean
roughly the same thing. And then you might carve out some final
portion of it to be your test set. And so the workflow is that you keep on
training algorithms on your training set. And use your dev set or your hold-out
cross validation set to see which of many different models
performs best on your dev set. And then after having
done this long enough, when you have a final model
that you want to evaluate, you can take the best model you have
found and evaluate it on your test set in order to get an unbiased estimate
of how well your algorithm is doing. So in the previous era of machine
learning, it was common practice to take all your data and
split it according to maybe a 70/30% in terms of a...people often talk about
the 70/30 train test splits. If you don't have an explicit dev set or
maybe a 60/20/20% split, in terms of 60% train,
20% dev and 20% test. And several years ago, this was
widely considered best practice in machine learning. If you have here maybe 100 examples in total, maybe 1000 examples in total,
maybe after 10,000 examples, these sorts of ratios were perfectly
reasonable rules of thumb. But in the modern big data era,
where, for example, you might have a million examples in
total, then the trend is that your dev and test sets have been becoming a much
smaller percentage of the total. Because remember, the goal of the dev set
or the development set is that you're going to test different algorithms on it
and see which algorithm works better. So the dev set just needs
to be big enough for you to evaluate, say,
two different algorithm choices or ten different algorithm choices and
quickly decide which one is doing better. And you might not need a whole
20% of your data for that. So, for example, if you have a million
training examples, you might decide that just having 10,000 examples in
your dev set is more than enough to evaluate, you know, which one or
two algorithms does better. And in a similar vein, the main goal
of your test set is, given your final classifier, to give you a pretty confident
estimate of how well it's doing. And again, if you have a million examples,
maybe you might decide that 10,000 examples is more than enough in order
to evaluate a single classifier and give you a good estimate
of how well it's doing. So, in this example, where you
have a million examples, if you need just 10,000 for
your dev and 10,000 for your test, your ratio will be more like...this
10,000 is 1% of 1 million, so you'll have 98% train, 1% dev, 1% test. And I've also seen applications where, if you have even more than a million
examples, you might end up with, you know, 99.5% train and
0.25% dev, 0.25% test. Or maybe a 0.4% dev, 0.1% test. So just to recap, when setting up
your machine learning problem, I'll often set it up into a train,
dev and test sets, and if you have a relatively small dataset,
these traditional ratios might be okay. But if you have a much larger data set,
it's also fine to set your dev and test sets to be much smaller than
your 20% or even 10% of your data. We'll give more specific
guidelines on the sizes of dev and test sets later in this specialization. One other trend we're seeing in the era
of modern deep learning is that more and more people train on mismatched train and
test distributions. Let's say you're building an app that
lets users upload a lot of pictures and your goal is to find pictures of
cats in order to show your users. Maybe all your users are cat lovers. Maybe your training set comes from cat
pictures downloaded off the Internet, but your dev and test sets might comprise
cat pictures from users using your app. So maybe your training set has a lot of
pictures crawled off the Internet but the dev and
test sets are pictures uploaded by users. Turns out a lot of webpages have very
high resolution, very professional, very nicely framed pictures of cats. But maybe your users
are uploading, you know, blurrier, lower res images just taken with a cell
phone camera in a more casual condition. And so these two distributions
of data may be different. The rule of thumb I'd encourage
you to follow, in this case, is to make sure that the dev and
test sets come from the same distribution. We'll say more about this
particular guideline as well, but because you will be using the dev set to
evaluate a lot of different models and trying really hard to improve
performance on the dev set, it's nice if your dev set comes from
the same distribution as your test set. But because deep learning algorithms have
such a huge hunger for training data, one trend I'm seeing is that you might
use all sorts of creative tactics, such as crawling webpages, in order to acquire a much bigger training
set than you would otherwise have. Even if part of the cost of that
is then that your training set data might not come from the same
distribution as your dev and test sets. But you find that so
long as you follow this rule of thumb, that progress in your machine
learning algorithm will be faster. And I'll give a more
detailed explanation for this particular rule of thumb later
in the specialization as well. Finally, it might be okay
to not have a test set. Remember, the goal of the test set
is to give you a ... unbiased estimate of the performance of your final network,
of the network that you selected. But if you don't need
that unbiased estimate, then it might be okay
to not have a test set. So what you do, if you have only
a dev set but not a test set, is you train on the training set and then
you try different model architectures. Evaluate them on the dev set,
and then use that to iterate and try to get to a good model. Because you've fit your
data to the dev set, this no longer gives you an unbiased
estimate of performance. But if you don't need one,
that might be perfectly fine. In the machine learning world,
when you have just a train and a dev set but no separate test set, most people will call
this a training set and they will call the dev set the test set. But what they actually end up doing is
using the test set as a hold-out cross validation set. Which maybe isn't completely
a great use of terminology, because they're then
overfitting to the test set. So when the team tells you that they
have only a train and a test set, I would just be cautious and think,
do they really have a train dev set? Because they're overfitting
to the test set. Culturally, it might be difficult to
change some of these team's terminology and get them to call it a trained dev
set rather than a trained test set, even though I think calling it a train and development set would be
more correct terminology. And this is actually okay practice
if you don't need a completely unbiased estimate of
the performance of your algorithm. So having set up a train dev and test set
will allow you to integrate more quickly. It will also allow you to more efficiently
measure the bias and variance of your algorithm so you can more efficiently
select ways to improve your algorithm. Let's start to talk about
that in the next video.
---


Lecture-2
---
I've noticed that almost all the really
good machine learning practitioners tend to have a very sophisticated
understanding of bias and variance. Bias and variance is one of those
concepts that's easy to learn but difficult to master. Even if you think you've seen
the basic concepts of bias and variance is often more nuanced
to it than you'd expect. In the deep learning era, another trend
is that there's been less discussion of what's called the bias variance trade off. You might have heard this thing called
the bias variance trade off, but in the deep learning era,
there's less of a trade off. So we still talk about bias,
we still talk about variance, but we just talk less about
the bias variance trade off. Let's see what this means. Let's say you have a data
set that looks like this. If you fit a straight line to the data, maybe you get a logistic
regression fit to that. This is not a very good fit to the data,
and so there's a cause of high bias. Or we say that this is
underfitting the data. On the opposite end, if you fit
an incredibly complex classifier, maybe a deep neural network. Or a new network with
a lot of hidden units, maybe you can fit the data perfectly. But that doesn't look
like a great fit either. So this is a classifier
with high variance, and this is overfitting the data. And there might be some classifier
in between with a medium level of complexity that maybe
fits a curve like that. That looks like a much more
reasonable fit to the data. So that's the, and call that just
right somewhere in each tree. So in a 2d example like this,
with just two features, x1 and x2, you can plot the data and
visualize bias and variance. In high dimensional problems,
you can't plot the data and visualize the decision boundary. Instead, there are couple different
metrics that we'll look at to try to understand bias and variance. So, continuing our example of
cat picture classification, where that's a positive example and
that's a negative example. The two key numbers to look
at to understand bias and variance will be the trading set error and
the dev set, or the development set error. So, for the sake of argument,
let's say that recognizing cats in pictures is something that
people can do nearly perfectly, right? And so let's say your trading size
error is 1% and your dev set error is, for the sake of argument,
let's say, is 11%. So in this example, you're doing
very well on the training set, but you're doing relatively poorly
on the development set. So this looks like you might
have overfit the training set. That somehow you're not generalizing well
to this holdout cost validation set to development set. And so if you have an example like this,
we will say this has high variance. So by looking at the training set
error and the development set error, you would be able to render a diagnosis
of your algorithm having high variance. Now let's say that you measure your
training set in your dev set error and you get a different result. Let's say that your
training set error is 15%. I'm writing your training set error in
the top row and your dev set error is 16%. In this case, assuming that
humans achieve roughly 0% error, that humans can look at these pictures and
just tell if it's cat or not. Then it looks like the algorithm is not
even doing very well on the training set. So if it's not even fitting
the training data, as seen that well, then this is underfitting the data. And so this algorithm has high bias. But in contrast, this is actually generalizing at
a reasonable level to the dev set, whereas performance of the dev set is only 1%
worse as performance on the training set. So this algorithm has a problem of high
bias because it's not even training, it's not even fitting
the training set well. This is similar to the leftmost
plot we had on the previous slide. Now here's another example. Let's say that you have
15% trading set error. So that's pretty high bias. But when you evaluate on a dev set,
it does even worse., maybe it does 30%. In this case, I would diagnose
this algorithm as having high bias because it's not doing that well on
the trading set and high variance. So this is really
the worst of both worlds. And one last example, if you have 0.5
training set error and 1% dev set error. Then maybe our users are quite happy that
you have a cad costly with only 1% error, then this would have low bias and
low variance. One subtlety that I'll
just briefly mention, but we'll leave to a later
video to discuss in detail. Is that this analysis is predicated
on the assumption that human level performance gets nearly 0% error. Or more generally they're the optimal
error, sometimes called Bayes error for the so
the bayesian optimal error is nearly 0%. I don't want to go into detail on
this in this particular video. But it turns out that if the optimal error
or the Bayes error were much higher, say it were 15%. Then if you look at this classifier,
15% is actually perfectly reasonable for training set. And you wouldn't say it as high bias and
also have pretty low variance. So the case of how to analyze bias and variance when no classifier
can do very well. For example,
if you have really blurry images so that even a human or just no system
could possibly do very well. Then maybe Bayes error is much higher. And then there's some details of
how this analysis will change. But leaving aside this subtlety for now, the takeaway is that by looking
at your trading set error. You can get a sense of how well you're
fitting at least the training data. And so
that tells you if you have a bias problem. And then looking at how much higher your
error goes when you go from the training set to the dev set. That should give you a sense of
how bad is the variance problem. So are you doing a good job generalizing
from the training set to the dev set that gives you a sense of your variance? All this is under the assumption that
the Bayes error is quite small and that your train and your death sets
are drawn from the same distribution. If those assumptions are violated, there's
more sophisticated analysis you could do, which we'll talk about in the later video. Now, on the previous slide you saw what
high bias, high variance looks like, and I guess you have the sense of what
a good classifier looks like. What does high bias and
high variance looks like? It's kind of the worst of both worlds. So you remember we said that
a classifier like this, a linear classifier, has high bias
because it under fits the data. So this would be a classifier
that is mostly linear and therefore under fits the data. We'll join this in purple. But if somehow your classifier
does some weird things, then it's actually overfitting
parts of the data as well. So the classifier that I drew in purple
has both high bias and high variance. There's high bias because by
being a mostly linear classifier, it's just not fitting this
quadratic light shape that well. But by having too much flexibility in
the middle, it somehow gets this example. And this example overfits
those two examples as well. So this classifier kind of has high
bias because it was mostly linear, but you needed maybe a curve function,
a quadratic function. And it has high variance because it had
too much flexibility to fit those two mislabeled outlier examples
in the middle as well. In case this seems contrived, well, it is. This example is a little bit
contrived in two dimensions, but with very high dimensional inputs. You actually do get things with
high bias in some regions and high variance in some regions. And so it is possible to get cross files
like this in high dimensional inputs that seem less contrived. So, to summarize, you've seen how by
looking at your algorithm's error on the training set and
your algorithm's error on the dev set. You can try to diagnose whether it has
problem of high bias or high variance, or maybe both, or maybe neither. And depending on whether your algorithm
suffers from bias or variance, it turns out that there
are different things you could try. So in the next video, I want to present
to you what I call a basic recipe for machine learning. That lets you more systematically try
to improve your algorithm depending on whether as high bias or
high variance issues. So let's go on to the next video.
---


Lecture-3
---
In the previous video, you saw how looking at training error and depth error can help you diagnose whether your algorithm has a bias or a variance problem, or maybe both. It turns out that this information that lets you much more systematically, using what they call a basic recipe for machine learning and lets you much more systematically go about improving your algorithms' performance. Let's take a look. When training a neural network, here's a basic recipe I will use. After having trained in an initial model, I will first ask, does your algorithm have high bias? And so, to try and evaluate if there is high bias, you should look at, really, the training set or the training data performance. Right. And so, if it does have high bias, does not even fitting in the training set that well, some things you could try would be to try pick a network, such as more hidden layers or more hidden units, or you could train it longer, you know, maybe run trains longer or try some more advanced optimization algorithms, which we'll talk about later in this course. Or, you can also try, this is kind of a, maybe it work, maybe it won't. But we'll see later that there are a lot of different neural network architectures and maybe you can find a new network architecture that's better suited for this problem. Putting this in parentheses because one of those things that, you know, you just have to try, maybe you can make it work, maybe not. Whereas, getting a bigger network almost always helps, and training longer, well, doesn't always help, but it certainly never hurts. But,so when training a learning algorithm, I would try these things until I can at least get rid of the bias problems, as I go back after I've tried this until, and keep doing that until I can fit, at least, fit the training set pretty well. And usually, if you have a big enough network, you should usually be able to fit the training data well, so long as it's a problem that is possible for someone to do, alright? If the image is very blurry, it may be impossible to fit it, but if at least a human can do well on the task, if you think Bayes error is not too high, then by training a big enough network you should be able to, hopefully, do well, at least on the training set, to at least fit or overfit the training set. Once you've reduce bias to acceptable amounts, I will then ask, do you have a variance problem? And so to evaluate that I would look at dev set performance. Are you able to generalize, from a pretty good training set performance, to having a pretty good dev set performance? And if you have high variance, well, best way to solve a high variance problem is to get more data, if you can get it, this, you know, can only help. But sometimes you can't get more data. Or, you could try regularization, which we'll talk about in the next video, to try to reduce overfitting. And then also, again, sometimes you just have to try it. But if you can find a more appropriate neural network architecture, sometimes that can reduce your variance problem as well, as well as reduce your bias problem. But how to do that? It's harder to be totally systematic how you do that. But, so I try these things and I kind of keep going back, until, hopefully, you find something with both low bias and low variance, whereupon you would be done. So a couple of points to notice. First, is that depending on whether you have high bias or high variance, the set of things you should try could be quite different. So I'll usually use the training dev set to try to diagnose if you have a bias or variance problem, and then use that to select the appropriate subset of things to try. So, for example, if you actually have a high bias problem, getting more training data is actually not going to help. Or, at least it's not the most efficient thing to do, alright? So being clear on how much of a bias problem or variance problem or both, can help you focus on selecting the most useful things to try. Second, in the earlier era of machine learning, there used to be a lot of discussion on what is called the bias variance tradeoff. And the reason for that was that, for a lot of the things you could try, you could increase bias and reduce variance, or reduce bias and increase variance. But, back in the pre-deep learning era, we didn't have many tools, we didn't have as many tools that just reduce bias, or that just reduce variance without hurting the other one. But in the modern deep learning, big data era, so long as you can keep training a bigger network, and so long as you can keep getting more data, which isn't always the case for either of these, but if that's the case, then getting a bigger network almost always just reduces your bias, without necessarily hurting your variance, so long as you regularize appropriately. And getting more data, pretty much always reduces your variance and doesn't hurt your bias much. So what's really happened is that, with these two steps, the ability to train, pick a network, or get more data, we now have tools to drive down bias and just drive down bias, or drive down variance and just drive down variance, without really hurting the other thing that much. And I think this has been one of the big reasons that deep learning has been so useful for supervised learning, that there's much less of this tradeoff where you have to carefully balance bias and variance, but sometimes, you just have more options for reducing bias or reducing variance, without necessarily increasing the other one. And, in fact, so last, you have a well-regularized network. We'll talk about regularization starting from the next video. Training a bigger network almost never hurts. And the main cost of training a neural network that's too big is just computational time, so long as you're regularizing. So I hope this gives you a sense of the basic structure of how to organize your machine learning problem to diagnose bias and variance, and then try to select the right operation for you to make progress on your problem. One of the things I mentioned several times in the video is regularization, is a very useful technique for reducing variance. There is a little bit of a bias variance tradeoff when you use regularization. It might increase the bias a little bit, although often not too much if you have a huge enough network. But, let's dive into more details in the next video so you can better understand how to apply regularization to your neural network.
---


Lecture-4
---
If you suspect your neural network
is over fitting your data, that is, you have a high variance problem, one of the first things you should
try is probably regularization. The other way to address high variance is to get more training data
that's also quite reliable. But you can't always get
more training data, or it could be expensive to get more data. But adding regularization will often
help to prevent overfitting, or to reduce variance in your network. So let's see how regularization works. Let's develop these ideas
using logistic regression. Recall that for logistic regression,
you try to minimize the cost function J, which is defined as this cost function. Some of your training examples of the
losses of the individual predictions in the different examples,
where you recall that w and b in the logistic regression,
are the parameters. So w is an x-dimensional parameter vector,
and b is a real number. And so, to add regularization to
logistic regression, what you do is add to it, this thing, lambda, which is
called the regularization parameter. I'll say more about that in a second. But lambda over 2m times the norm of w squared. So here, the norm of w squared, is just equal to sum from j equals 1 to nx of wj squared,
or this can also be written w, transpose w, it's just a square
Euclidean norm of the prime to vector w. And this is called L2 regularization. Because here,
you're using the Euclidean norm, also it's called the L2 norm with
the parameter vector w. Now, why do you regularize
just the parameter w? Why don't we add something
here, you know, about b as well? In practice, you could do this,
but I usually just omit this. Because if you look at your parameters,
w is usually a pretty high dimensional parameter vector,
especially with a high variance problem. Maybe w just has a lot of parameters, so you aren't fitting all the parameters
well, whereas b is just a single number. So almost all the parameters
are in w rather than b. And if you add this last term,
in practice, it won't make much of a difference, because b is just one parameter over
a very large number of parameters. In practice,
I usually just don't bother to include it. But you can if you want. So L2 regularization is the most
common type of regularization. You might have also heard of some
people talk about L1 regularization. And that's when you add,
instead of this L2 norm, you instead add a term that is
lambda over m of sum over, of this. And this is also called the L1
norm of the parameter vector w, so the little subscript 1 down there,
right? And I guess whether you put m or 2m in the
denominator, is just a scaling constant. If you use L1 regularization,
then w will end up being sparse. And what that means is that the w
vector will have a lot of zeros in it. And some people say that this can help
with compressing the model, because the set of parameters are zero, then
you need less memory to store the model. Although, I find that, in practice, L1
regularization, to make your model sparse, helps only a little bit. So I don't think it's used that much,
at least not for the purpose of compressing your model. And when people train your networks, L2 regularization is just
used much, much more often. (Sorry, just fixing up some
of the notation here). So, one last detail. Lambda here is called the regularization
parameter. And usually, you set this
using your development set, or using hold-out cross validation. When you try a variety of values and
see what does the best, in terms of trading off between doing
well in your training set versus also setting that two normal of
your parameters to be small, which helps prevent over fitting. So lambda is another hyper parameter
that you might have to tune. And by the way, for
the programming exercises, lambda is a reserved keyword in
the Python programming language. So in the programming exercise,
we will have l-a-m-b-d, without the a, so as not to clash
with the reserved keyword in Python. So we use l-a-m-b-d to represent
the lambda regularization parameter. So this is how you implement L2
regularization for logistic regression. How about a neural network? In a neural network, you have a cost
function that's a function of all of your parameters, w[1],
b[1] through w[capital L], b[capital L], where capital L is the number of
layers in your neural network. And so the cost function is this,
sum of the losses, sum over your m training examples. And so to add regularization,
you add lambda over 2m, of sum over all of your parameters w,
your parameter matrix is w, of their, that's called the squared norm. Where, this norm of a matrix,
really the squared norm, is defined as the sum
of i, sum of j, of each of the elements of that matrix,
squared. And if you want the indices
of this summation, this is sum from i=1 through n[l minus 1]. Sum from j=1 through n[l], because w is a n[l] by n[l minus 1]
dimensional matrix, where these are the number of
hidden units or number of units
in layers [l minus 1] in layer l. So this matrix norm,
it turns out is called the Frobenius norm of the matrix,
denoted with a F in the subscript. So for
arcane linear algebra technical reasons, this is not called the, you know, l2
norm of a matrix. Instead, it's called
the Frobenius norm of a matrix. I know it sounds like it would be more
natural to just call the l2 norm of the matrix, but for really arcane
reasons that you don't need to know, by convention,
this is called the Frobenius norm. It just means the sum of square
of elements of a matrix. So how do you implement
gradient descent with this? Previously, we would
complete dw, you know, using backprop, where backprop would give
us the partial derivative of J with respect to w, or
really w for any given [l]. And then you update w[l],
as w[l] minus the learning rate, times d. So this is before we added this extra
regularization term to the objective. Now that we've added this
regularization term to the objective, what you do is you take dw and
you add to it, lambda over m times w. And then you just compute this update,
same as before. And it turns out that with
this new definition of dw[l], this is still, you know, this new dw[l] is still a correct
definition of the derivative of your cost function,
with respect to your parameters, now that you've added the extra
regularization term at the end. And it's for this reason that L2
regularization is sometimes also called weight decay. So if I take this definition of dw[l] and
just plug it in here, then you see that the update
is w[l] gets updated as w[l] times the learning rate alpha times, you know,
the thing from backprop, plus lambda over m, times w[l]. Let's move the minus sign there. And so this is equal to w[l] minus alpha, lambda over m times w[l], minus alpha times, you know, the thing you got from backprop. And so this term shows that
whatever the matrix w[l] is, you're going to make it
a little bit smaller, right? This is actually as if you're
taking the matrix w and you're multiplying it by 1 minus alpha lambda over m. You're really taking the matrix w and
subtracting alpha lambda over m times this. Like you're multiplying the
matrix w by this number, which is going to be
a little bit less than 1. So this is why L2 norm regularization
is also called weight decay. Because it's just like the ordinary
gradient descent, where you update w by subtracting alpha, times the original
gradient you got from backprop. But now you're also, you know,
multiplying w by this thing, which is a little bit less than 1. So the alternative name for
L2 regularization is weight decay. I'm not really going to use that name,
but the intuition for why it's called weight decay is that this
first term here, is equal to this. So you're just multiplying the weight
matrix by a number slightly less than 1. So that's how you implement L2
regularization in a neural network. Now, one question that peer centers
ask me is, you know, "Hey, Andrew, why does regularization
prevent over-fitting?" Let's take a quick look at the next video, and gain some intuition for
how regularization prevents over-fitting.
---


Lecture-5
---
Why does regularization help with overfitting? Why does it help with reducing variance problems? Let's go through a couple examples to gain some intuition about how it works. So, recall that our high bias, high variance, and "just write" pictures from our earlier video had looked something like this. Now, let's see a fitting large and deep neural network. I know I haven't drawn this one too large or too deep, but let's see if [INAUDIBLE] some neural network and is currently overfitting. So you have some cost function, write J of W, b equals sum of the losses, like so, right? And so what we did for regularization was add
this extra term that penalizes the weight matrices from being too large. And we said that was the Frobenius norm. So why is it that shrinking the L2 norm, or the Frobenius norm with the parameters might cause less overfitting? One piece of intuition is that if you, you know, crank your regularization lambda to be really, really big, that'll be really incentivized to set the weight matrices, W, to be reasonably close to zero. So one piece of intuition is maybe it'll set the weight to be so close to zero for a lot of hidden units that's basically zeroing out a lot of the impact of these hidden units. And if that's the case, then, you know, this much simplified neural network becomes a much smaller neural network. In fact, it is almost like a logistic regression unit, you know, but stacked multiple layers deep. And so that will take you from this overfitting case, much closer to the left, to the other high bias case. But, hopefully, there'll be an intermediate value of lambda that results in the result closer to this "just right" case in the middle. But the intuition is that by cranking up lambda to be really big, it'll set W close to zero, which, in practice, this isn't actually what happens. We can think of it as zeroing out, or at least reducing, the impact of a lot of the hidden units, so you end up with what might feel like a simpler network, that gets closer and closer as if you're just using logistic regression. The intuition of completely zeroing out a bunch of hidden units isn't quite right. It turns out that what actually happens is it'll still use all the hidden units, but each of them would just have a much smaller effect. But you do end up with a simpler network, and as if you have a smaller network that is, therefore, less prone to overfitting. So I'm not sure if this intuition helps, but when you implement regularization in the program exercise, you actually see some of these variance reduction results yourself. Here's another attempt at additional intuition for why regularization helps prevent overfitting. And for this, I'm going to assume that we're using the tan h activation function, which looks like this. This is g of z equals tan h of z. So if that's the case, notice that so long as z is quite small, so if z takes on only a smallish range of parameters, maybe around here, then you're just using the linear regime of the tan h function, is only if z is allowed to wander, you know, to larger values or smaller values like so, that the activation function starts to become less linear. So the intuition you might take away from this is that if lambda, the regularization parameter is large, then you have that your parameters will be relatively small, because they are penalized being large in the cost function. And so if the weights, W, are small, then because z is equal to W, right, and then technically, it's plus b. But if W tends to be very small, then z will also be relatively small. And in particular, if z ends up taking relatively small values, just in this little range, then g of z will be roughly linear. So it's as if every layer will be roughly linear, as if it is just linear regression. And we saw in course one that if every layer is linear, then your whole network is just a linear network. And so even a very deep network, with a deep network with a linear activation function is, at the end of the day, only able to compute a linear function. So it's not able to, you know, fit those very, very complicated decision, very non-linear decision boundaries that allow it to, you know, really overfit, right, to data sets, like we saw on the overfitting high variance case on the previous slide, ok? So just to summarize, if the regularization parameters are very large, the parameters W very small, so z will be relatively small, kind of ignoring the effects of b for now, but so z is relatively, so z will be relatively small, or really, I should say it takes on a small range of values. And so the activation function if it's tan h, say, will be relatively linear. And so your whole neural network will be computing something not too far from a big linear function, which is therefore, pretty simple function, rather than a very complex highly non-linear function. And so, is also much less able to overfit, ok? And again, when you implement regularization for yourself in the program exercise, you'll be able to see some of these effects yourself. Before wrapping up our def discussion on regularization, I just want to give you one implementational tip, which is that, when implementing regularization, we took our definition of the cost function J and we actually modified it by adding this extra term that penalizes the weights being too large. And so if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost function J, as a function of the number of elevations of gradient descent, and you want to see that the cost function J decreases monotonically after every elevation of gradient descent. And if you're implementing regularization, then please remember that J now has this new definition. If you plot the old definition of J, just this first term, then you might not see a decrease monotonically. So to debug gradient descent, make sure that you're plotting, you know, this new definition of J that includes this second term as well. Otherwise, you might not see J decrease monotonically on every single elevation. So that's it for L2 regularization, which is actually a regularization technique that I use the most in training deep learning models. In deep learning, there is another sometimes used regularization technique called dropout regularization. Let's take a look at that in the next video.
---


Lecture-6
---
In addition to L2 regularization, another very powerful regularization techniques is called "dropout." Let's see how that works. Let's say you train a neural network like the one on the left and there's over-fitting. Here's what you do with dropout. Let me make a copy of the neural network. With dropout, what we're going to do is go through each of the layers of the network and set some probability of eliminating a node in neural network. Let's say that for each of these layers, we're going to- for each node, toss a coin and have a 0.5 chance of keeping each node and 0.5 chance of removing each node. So, after the coin tosses, maybe we'll decide to eliminate those nodes, then what you do is actually remove all the outgoing things from that no as well. So you end up with a much smaller, really much diminished network. And then you do back propagation training. There's one example on this much diminished network. And then on different examples, you would toss a set of coins again and keep a different set of nodes and then dropout or eliminate different than nodes. And so for each training example, you would train it using one of these neural based networks. So, maybe it seems like a slightly crazy technique. They just go around coding those are random, but this actually works. But you can imagine that because you're training a much smaller network on each example or maybe just give a sense for why you end up able to regularize the network, because these much smaller networks are being trained. Let's look at how you implement dropout. There are a few ways of implementing dropout. I'm going to show you the most common one, which is technique called inverted dropout. For the sake of completeness, let's say we want to illustrate this with layer l=3. So, in the code I'm going to write- there will be a bunch of 3s here. I'm just illustrating how to represent dropout in a single layer. So, what we are going to do is set a vector d and d^3 is going to be the dropout vector for the layer 3. That's what the 3 is to be np.random.rand(a). And this is going to be the same shape as a3. And when I see if this is less than some number, which I'm going to call keep.prob. And so, keep.prob is a number. It was 0.5 on the previous time, and maybe now I'll use 0.8 in this example, and there will be the probability that a given hidden unit will be kept. So keep.prob = 0.8, then this means that there's a 0.2 chance of eliminating any hidden unit. So, what it does is it generates a random matrix. And this works as well if you have factorized. So d3 will be a matrix. Therefore, each example have a each hidden unit there's a 0.8 chance that the corresponding d3 will be one, and a 20% chance there will be zero. So, this random numbers being less than 0.8 it has a 0.8 chance of being one or be true, and 20% or 0.2 chance of being false, of being zero. And then what you are going to do is take your activations from the third layer, let me just call it a3 in this low example. So, a3 has the activations you computate. And you can set a3 to be equal to the old a3, times- There is element wise multiplication. Or you can also write this as a3* = d3. But what this does is for every element of d3 that's equal to zero. And there was a 20% chance of each of the elements being zero, just multiply operation ends up zeroing out, the corresponding element of d3. If you do this in python, technically d3 will be a boolean array where value is true and false, rather than one and zero. But the multiply operation works and will interpret the true and false values as one and zero. If you try this yourself in python, you'll see. Then finally, we're going to take a3 and scale it up by dividing by 0.8 or really dividing by our keep.prob parameter. So, let me explain what this final step is doing. Let's say for the sake of argument that you have 50 units or 50 neurons in the third hidden layer. So maybe a3 is 50 by one dimensional or if you- factorization maybe it's 50 by m dimensional. So, if you have a 80% chance of keeping them and 20% chance of eliminating them. This means that on average, you end up with 10 units shut off or 10 units zeroed out. And so now, if you look at the value of z^4, z^4 is going to be equal to w^4 * a^3 + b^4. And so, on expectation, this will be reduced by 20%. By which I mean that 20% of the elements of a3 will be zeroed out. So, in order to not reduce the expected value of z^4, what you do is you need to take this, and divide it by 0.8 because this will correct or just a bump that back up by roughly 20% that you need. So it's not changed the expected value of a3. And, so this line here is what's called the inverted dropout technique. And its effect is that, no matter what you set to keep.prob to, whether it's 0.8 or 0.9 or even one, if it's set to one then there's no dropout, because it's keeping everything or 0.5 or whatever, this inverted dropout technique by dividing by the keep.prob, it ensures that the expected value of a3 remains the same. And it turns out that at test time, when you trying to evaluate a neural network, which we'll talk about on the next slide, this inverted dropout technique, There's this slide, just green box around the next test This makes test time easier because you have less of a scaling problem. By far the most common implementation of dropouts today as far as I know is inverted dropouts. I recommend you just implement this. But there were some early iterations of dropout that missed this divide by keep.prob line, and so at test time the average becomes more and more complicated. But again, people tend not to use those other versions. So, what you do is you use the d vector, and you'll notice that for different training examples, you zero out different hidden units. And in fact, if you make multiple passes through the same training set, then on different pauses through the training set, you should randomly zero out different hidden units. So, it's not that for one example, you should keep zeroing out the same hidden units is that, on iteration one of grade and descent, you might zero out some hidden units. And on the second iteration of great descent where you go through the training set the second time, maybe you'll zero out a different pattern of hidden units. And the vector d or d3, for the third layer, is used to decide what to zero out, both in for prob as well as in that prob. We are just showing for prob here. Now, having trained the algorithm at test time, here's what you would do. At test time, you're given some x or which you want to make a prediction. And using our standard notation, I'm going to use a^0, the activations of the zeroes layer to denote just test example x. So what we're going to do is not to use dropout at test time in particular which is in a sense. Z^1= w^1.a^0 + b^1. a^1 = g^1(z^1 Z). Z^2 = w^2.a^1 + b^2. a^2 =... And so on. Until you get to the last layer and that you make a prediction y^. But notice that the test time you're not using dropout explicitly and you're not tossing coins at random, you're not flipping coins to decide which hidden units to eliminate. And that's because when you are making predictions at the test time, you don't really want your output to be random. If you are implementing dropout at test time, that just add noise to your predictions. In theory, one thing you could do is run a prediction process many times with different hidden units randomly dropped out and have it across them. But that's computationally inefficient and will give you roughly the same result; very, very similar results to this different procedure as well. And just to mention, the inverted dropout thing, you remember the step on the previous line when we divided by the cheap.prob. The effect of that was to ensure that even when you don't see men dropout at test time to the scaling, the expected value of these activations don't change. So, you don't need to add in an extra funny scaling parameter at test time. That's different than when you have that training time. So that's dropouts. And when you implement this in week's premier exercise, you gain more firsthand experience with it as well. But why does it really work? What I want to do the next video is give you some better intuition about what dropout really is doing. Let's go on to the next video.
---


Lecture-7
---
Drop out. Does this seemingly crazy thing of
randomly knocking out units in your network? Why does it work? So as a regulizer,
let's give some better intuition. In the previous video, I gave this intuition that drop out
randomly knocks out units in your network. So it's as if on every iteration you're
working with a smaller neural network. And so using a smaller
neural network seems like it should have a regularizing effect. Here's the second intuition which is,
you know, let's look at it from
the perspective of a single unit. Right, let's say this one. Now for this
unit to do his job has four inputs and it needs to generate
some meaningful output. Now with drop out,
the inputs can get randomly eliminated. You know, sometimes those two
units will get eliminated. Sometimes a different
unit will get eliminated. So what this means is that this
unit which I'm circling purple. It can't rely on anyone feature because
anyone feature could go away at random or anyone of its own inputs
could go away at random. So in particular, I will be reluctant to put all of its
bets on say just this input, right. The ways were reluctant to put too much
weight on anyone input because it could go away. So this unit will be more motivated
to spread out this ways and give you a little bit of weight to
each of the four inputs to this unit. And by spreading out the weights
this will tend to have an effect of shrinking the squared
norm of the weights, and so similar to what we
saw with L2 regularization. The effect of implementing dropout
is that its strength the ways and similar to L2 regularization, it helps to
prevent overfitting, but it turns out that dropout can formally be shown to be
an adaptive form of L2 regularization, but the L2 penalty on different ways
are different depending on the size of the activation is being
multiplied into that way. But to summarize it is possible to show
that dropout has a similar effect to. L2 regularization. Only the L2 regularization applied to
different ways can be a little bit different and even more adaptive
to the scale of different inputs. One more detail for
when you're implementing dropout, here's a network where you
have three input features. This is seven hidden units here. 7, 3, 2, 1, so
one of the practice we have to choose was the keep prop which is a chance
of keeping a unit in each layer. So it is also feasible to
vary keep-propped by layer. So for the first layer,
your matrix W1 will be 7 by 3. Your second weight matrix will be 7 by 7. W3 will be 3 by 7 and so on. And so W2 is actually the biggest
weight matrix, right? Because they're actually
the largest set of parameters. B and W2, which is 7 by 7. So to prevent, to reduce overfitting
of that matrix, maybe for this layer, I guess this is layer 2, you might
have a key prop that's relatively low, say 0.5, whereas for different layers
where you might worry less about over 15, you could have a higher key problem. Maybe just 0.7, maybe this is 0.7. And then for layers we don't
worry about overfitting at all. You can have a key prop of 1.0. Right? So, you know, for clarity, these are
numbers I'm drawing in the purple boxes. These could be different key props for
different layers. Notice that the key problem 1.0 means
that you're keeping every unit. And so you're really not using
drop out for that layer. But for layers where you're more worried
about overfitting really the layers with a lot of parameters you could say keep
prop to be smaller to apply a more powerful form of dropout. It's kind of like cranking
up the regularization. Parameter lambda of L2 regularization
where you try to regularize some layers more than others. And technically you can also apply drop
out to the input layer where you can have some chance of just acting out one or
more of the input features, although in practice,
usually don't do that often. And so key problem of 1.0 is
quite common for the input there. You might also use a very high value,
maybe 0.9 but is much less likely that you want to eliminate half of the input
features so usually keep prop. If you apply that all will
be a number close to 1. If you even apply dropout
at all to the input layer. So just to summarize if you're more
worried about some layers of fitting than others, you can set a lower key prop for
some layers than others. The downside is this gives you even
more hyper parameters to search for using cross validation. One other alternative might be to have
some layers where you apply dropout and some layers where you
don't apply drop out and then just have one hyper parameter which
is a key prop for the layers for which you do apply drop out and before we wrap
up just a couple implantation all tips. Many of the first successful
implementations of dropouts were to computer vision, so
in computer vision, the input sizes so big in putting all these pixels that
you almost never have enough data. And so drop out is very frequently used
by the computer vision and there are some common vision research is that pretty
much always use it almost as a default. But really, the thing to remember is that
drop out is a regularization technique, it helps prevent overfitting. And so unless my avram is overfitting, I
wouldn't actually bother to use drop out. So as you somewhat less often
in other application areas, there's just a computer vision,
you usually just don't have enough data so you almost always overfitting,
which is why they tend to be some computer vision researchers swear
by drop out by the intuition. I was, doesn't always generalize,
I think to other disciplines. One big downside of drop out
is that the cost function J is no longer well defined
on every iteration. You're randomly,
calling off a bunch of notes. And so if you are double checking
the performance of great inter sent is actually harder to double check that,
right? You have a well defined cost function J. That is going downhill on every
elevation because the cost function J. That you're optimizing is actually less. Less well defined or
it's certainly hard to calculate. So you lose this debugging tool
to have a plot a draft like this. So what I usually do is turn off drop out
or if you will set keep-propped = 1 and run my code and make sure that it
is monitored quickly decreasing J. And then turn on drop out and
hope that, I didn't introduce, welcome to my code during drop out
because you need other ways, I guess, but not plotting these figures to make
sure that your code is working, the greatest is working
even with drop out. So with that there's still a few more
regularization techniques that were feel knowing. Let's talk about a few more such
techniques in the next video.
---


Lecture-8
---
In addition to L2 regularization and
drop out regularization there are few other techniques to reducing
over fitting in your neural network. Let's take a look. Let's say you fitting a CAD crossfire. If you are over fitting getting more
training data can help, but getting more training data can be expensive and
sometimes you just can't get more data. But what you can do is augment your
training set by taking image like this. And for example,
flipping it horizontally and adding that also with your training set. So now instead of just this one
example in your training set, you can add this to your training example. So by flipping the images horizontally, you could double the size
of your training set. Because you're training set is now a bit
redundant this isn't as good as if you had collected an additional set of
brand new independent examples. But you could do this Without needing
to pay the expense of going out to take more pictures of cats. And then other than flipping horizontally, you can also take random
crops of the image. So here we're rotated and
sort of randomly zoom into the image and this still looks like a cat. So by taking random distortions and
translations of the image you could augment your data set and
make additional fake training examples. Again, these extra fake training examples
they don't add as much information as they were to call they get a brand new
independent example of a cat. But because you can do this,
almost for free, other than for some computational costs. This can be an inexpensive way to
give your algorithm more data and therefore sort of regularize it and
reduce over fitting. And by synthesizing examples like this
what you're really telling your algorithm is that If something is a cat then
flipping it horizontally is still a cat. Notice I didn't flip it vertically, because maybe we don't want
upside down cats, right? And then also maybe randomly zooming
in to part of the image it's probably still a cat. For optical character recognition you can
also bring your data set by taking digits and imposing random rotations and
distortions to it. So If you add these things
to your training set, these are also still digit force. For illustration I applied
a very strong distortion. So this look very wavy for, in practice
you don't need to distort the four quite as aggressively, but just a more subtle
distortion than what I'm showing here, to make this example clearer for
you, right? But a more subtle distortion
is usually used in practice, because this looks like
really warped fours. So data augmentation can be used
as a regularization technique, in fact similar to regularization. There's one other technique that is
often used called early stopping. So what you're going to do is as you run
gradient descent you're going to plot your, either the training error, you'll use 01 classification
error on the training set. Or just plot the cost
function J optimizing, and that should decrease monotonically,
like so, all right? Because as you trade, hopefully, you're trading around your cost
function J should decrease. So with early stopping,
what you do is you plot this, and you also plot your dev set error. And again, this could be a classification
error in a development sense, or something like the cost function, like the logistic
loss or the log loss of the dev set. Now what you find is that your dev
set error will usually go down for a while, and
then it will increase from there. So what early stopping does is,
you will say well, it looks like your neural network was
doing best around that iteration, so we just want to stop trading on
your neural network halfway and take whatever value achieved
this dev set error. So why does this work? Well when you've haven't
run many iterations for your neural network yet
your parameters w will be close to zero. Because with random initialization you
probably initialize w to small random values so before you train for
a long time, w is still quite small. And as you iterate, as you train, w will
get bigger and bigger and bigger until here maybe you have a much larger value of
the parameters w for your neural network. So what early stopping does is by
stopping halfway you have only a mid-size rate w. And so similar to L2 regularization by
picking a neural network with smaller norm for your parameters w, hopefully
your neural network is over fitting less. And the term early stopping refers
to the fact that you're just stopping the training of
your neural network earlier. I sometimes use early stopping
when training a neural network. But it does have one downside,
let me explain. I think of the machine learning process
as comprising several different steps. One, is that you want an algorithm
to optimize the cost function j and we have various tools to do that,
such as grade intersect. And then we'll talk later about
other algorithms, like momentum and RMS prop and Atom and so on. But after optimizing the cost function j,
you also wanted to not over-fit. And we have some tools to do that
such as your regularization, getting more data and so on. Now in machine learning, we already have
so many hyper-parameters it surge over. It's already very complicated to choose
among the space of possible algorithms. And so I find machine learning
easier to think about when you have one set of tools for
optimizing the cost function J, and when you're focusing on
authorizing the cost function J. All you care about is finding w and b,
so that J(w,b) is as small as possible. You just don't think about anything
else other than reducing this. And then it's completely
separate task to not over fit, in other words, to reduce variance. And when you're doing that, you have
a separate set of tools for doing it. And this principle is sometimes
called orthogonalization. And there's this idea, that you want to be
able to think about one task at a time. I'll say more about orthorganization
in a later video, so if you don't fully get the concept yet,
don't worry about it. But, to me the main downside
of early stopping is that this couples these two tasks. So you no longer can work on
these two problems independently, because by stopping gradient decent early, you're sort of breaking whatever you're
doing to optimize cost function J, because now you're not doing a great
job reducing the cost function J. You've sort of not done that that well. And then you also simultaneously
trying to not over fit. So instead of using different
tools to solve the two problems, you're using one that
kind of mixes the two. And this just makes the set of things you could try are more
complicated to think about. Rather than using early stopping, one
alternative is just use L2 regularization then you can just train the neural
network as long as possible. I find that this makes the search space
of hyper parameters easier to decompose, and easier to search over. But the downside of this though is that
you might have to try a lot of values of the regularization parameter lambda. And so this makes searching over many
values of lambda more computationally expensive. And the advantage of early stopping is
that running the gradient descent process just once, you get to try out
values of small w, mid-size w, and large w, without needing to try a lot
of values of the L2 regularization hyperparameter lambda. If this concept doesn't completely make
sense to you yet, don't worry about it. We're going to talk about
orthogonalization in greater detail in a later video,
I think this will make a bit more sense. Despite it's disadvantages,
many people do use it. I personally prefer to just
use L2 regularization and try different values of lambda. That's assuming you can afford
the computation to do so. But early stopping does let you
get a similar effect without needing to explicitly try lots
of different values of lambda. So you've now seen how to use data
augmentation as well as if you wish early stopping in order to reduce variance or
prevent over fitting your neural network. Next let's talk about some techniques for setting up your optimization problem
to make your training go quickly.
---


Lecture-9
---
When training a neural network, one of the techniques
to speed up your training is if you
normalize your inputs. Let's see what that means. Let's see the training sets
with two input features. The input features x
are two-dimensional and here's a scatter plot
of your training set. Normalizing your inputs
corresponds to two steps, the first is to subtract out
or to zero out the mean, so your sets mu equals 1 over m, sum over I of x_i. This is a vector and then x gets set as x minus mu for
every training example. This means that you just move the training set until
it has zero mean. Then the second step is to
normalize the variances. Notice here that
the feature x_1 has a much larger variance
than the feature x_2 here. What we do is set
sigma equals 1 over m sum of x_i star, star 2. I guess this is
element-y squaring. Now sigma squared is a vector with the variances of
each of the features. Notice we've already
subtracted out the mean, so x_i squared, element-y
square is just the variances. You take each example and
divide it by this vector sigma. In some pictures, you end
up with this where now the variance of x_1 and
x_2 are both equal to one. One tip. If you use this to
scale your training data, then use the same mu and sigma to normalize
your test set. In particular, you don't want to normalize the training set
and a test set differently. Whatever this value is and
whatever this value is, use them in these two formulas so that you scale
your test set in exactly the same way rather
than estimating mu and sigma squared separately on your training set and test set, because you want your data both training and test
examples to go through the same transformation
defined by the same Mu and Sigma squared calculated on your
training data. Why do we do this? Why do we want to normalize
the input features? Recall that the cost function is defined as written
on the top right. It turns out that if you use
unnormalized input features, it's more likely that your cost function
will look like this, like a very squished out bar, very elongated cost function where the minimum you're trying to find is
maybe over there. But if your features are
on very different scales, say the feature x_1 ranges from 1-1,000 and the feature
x_2 ranges from 0-1, then it turns out that the ratio or the
range of values for the parameters w_1 and w_2 will end up taking on
very different values. Maybe these axes
should be w_1 and w_2, but the intuition
of plot w and b, cost function can be very
elongated bow like that. If you plot the contours
of this function, you can have a very elongated
function like that. Whereas if you
normalize the features, then your cost function will on average look
more symmetric. If you are running
gradient descent on a cost function like
the one on the left, then you might
have to use a very small learning rate
because if you're here, the gradient decent might need a lot of steps to
oscillate back and forth before it finally finds
its way to the minimum. Whereas if you have more
spherical contours, then wherever you start, gradient descent can pretty much go straight to the minimum. You can take much larger steps where gradient descent need, rather than needing to oscillate around like
the picture on the left. Of course, in practice, w is a high dimensional vector. Trying to plot
this in 2D doesn't convey all the
intuitions correctly. But the rough intuition
that you cost function will be in a more round and easier to optimize when you're features are on similar scales. Not from 1-1000, 0-1, but mostly from minus 1-1 or about similar
variance as each other. That just makes
your cost function j easier and faster to optimize. In practice, if one feature, say x_1 ranges from 0-1 and
x_2 ranges from minus 1-1, and x_3 ranges from 1-2, these are fairly similar ranges, so this will work just fine, is when they are on dramatically
different ranges like ones from 1-1000 and
another from 0-1. That really hurts your
optimization algorithm. That by just setting all
of them to zero mean and say variance one like
we did on the last slide, that just guarantees that
all your features are in a similar scale and will usually help you learning
algorithm run faster. If your input features came
from very different scales, maybe some features
are from 0-1, sum from 1-1000, then it's important to normalize
your features. If your features came
in on similar scales, then this step is less
important although performing this type of normalization pretty much never does any harm. Often you'll do it anyway, if I'm not sure whether
or not it will help with speeding up training
for your algorithm. That's it for normalizing
your input features. Next, let's keep
talking about ways to speed up the training
of your neural network.
---


Lecture-10
---
One of the problems of training neural network, especially very deep neural networks, is data vanishing and exploding gradients. What that means is that when you're training a very deep network your derivatives or your slopes can sometimes get either very, very big or very, very small, maybe even exponentially small, and this makes training difficult. In this video you see what this problem of exploding and vanishing gradients really means, as well as how you can use careful choices of the random weight initialization to significantly reduce this problem. Unless you're training a very deep neural network like this, to save space on the slide, I've drawn it as if you have only two hidden units per layer, but it could be more as well. But this neural network will have parameters W1, W2, W3 and so on up to WL. For the sake of simplicity, let's say we're using an activation function G of Z equals Z, so linear activation function. And let's ignore B, let's say B of L equals zero. So in that case you can show that the output Y will be WL times WL minus one times WL minus two, dot, dot, dot down to the W3, W2, W1 times X. But if you want to just check my math, W1 times X is going to be Z1, because B is equal to zero. So Z1 is equal to, I guess, W1 times X and then plus B which is zero. But then A1 is equal to G of Z1. But because we use linear activation function, this is just equal to Z1. So this first term W1X is equal to A1. And then by the reasoning you can figure out that W2 times W1 times X is equal to A2, because that's going to be G of Z2, is going to be G of W2 times A1 which you can plug that in here. So this thing is going to be equal to A2, and then this thing is going to be A3 and so on until the protocol of all these matrices gives you Y-hat, not Y. Now, let's say that each of you weight matrices WL is just a little bit larger than one times the identity. So it's 1.5_1.5_0_0. Technically, the last one has different dimensions so maybe this is just the rest of these weight matrices. Then Y-hat will be, ignoring this last one with different dimension, this 1.5_0_0_1.5 matrix to the power of L minus 1 times X, because we assume that each one of these matrices is equal to this thing. It's really 1.5 times the identity matrix, then you end up with this calculation. And so Y-hat will be essentially 1.5 to the power of L, to the power of L minus 1 times X, and if L was large for very deep neural network, Y-hat will be very large. In fact, it just grows exponentially, it grows like 1.5 to the number of layers. And so if you have a very deep neural network, the value of Y will explode. Now, conversely, if we replace this with 0.5, so something less than 1, then this becomes 0.5 to the power of L. This matrix becomes 0.5 to the L minus one times X, again ignoring WL. And so each of your matrices are less than 1, then let's say X1, X2 were one one, then the activations will be one half, one half, one fourth, one fourth, one eighth, one eighth, and so on until this becomes one over two to the L. So the activation values will decrease exponentially as a function of the def, as a function of the number of layers L of the network. So in the very deep network, the activations end up decreasing exponentially. So the intuition I hope you can take away from this is that at the weights W, if they're all just a little bit bigger than one or just a little bit bigger than the identity matrix, then with a very deep network the activations can explode. And if W is just a little bit less than identity. So this maybe here's 0.9, 0.9, then you have a very deep network, the activations will decrease exponentially. And even though I went through this argument in terms of activations increasing or decreasing exponentially as a function of L, a similar argument can be used to show that the derivatives or the gradients the computer is going to send will also increase exponentially or decrease exponentially as a function of the number of layers. With some of the modern neural networks, L equals 150. Microsoft recently got great results with 152 layer neural network. But with such a deep neural network, if your activations or gradients increase or decrease exponentially as a function of L, then these values could get really big or really small. And this makes training difficult, especially if your gradients are exponentially smaller than L, then gradient descent will take tiny little steps. It will take a long time for gradient descent to learn anything. To summarize, you've seen how deep networks suffer from the problems of vanishing or exploding gradients. In fact, for a long time this problem was a huge barrier to training deep neural networks. It turns out there's a partial solution that doesn't completely solve this problem but it helps a lot which is careful choice of how you initialize the weights. To see that, let's go to the next video.
---


Lecture-11
---
In the last video you saw
how very deep neural networks can have the problems of
vanishing and exploding gradients. It turns out that a partial solution to this, doesn't solve it entirely but helps a lot, is better or more careful choice of the
random initialization for your neural network. To understand this, let's start with the
example of initializing the ways for a single neuron, and then we're go on to
generalize this to a deep network. Let's go through this with an example with just a single neuron, and then
we'll talk about the deep net later. So with a single neuron, you might input four
features, x1 through x4, and then you have some a=g(z) and then it outputs some y. And later on for a deeper net,
you know these inputs will be right, some layer a(l), but for now
let's just call this x for now. So z is going to be equal to
w1x1 + w2x2 +... + I guess WnXn. And let's set b=0 so, you know,
let's just ignore b for now. So in order to make z
not blow up and not become too small, you notice that
the larger n is, the smaller you want Wi to be, right? Because z is the sum of the WiXi. And so if you're adding up a lot of these terms,
you want each of these terms to be smaller. One reasonable thing to do would be to
set the variance of W to be equal to 1 over n, where n is the number of input features
that's going into a neuron. So in practice, what you can do is set
the weight matrix W for a certain layer to be np.random.randn you know, and then whatever the shape of the
matrix is for this out here, and then times square root of 1 over the number of features that
I fed into each neuron in layer l. So there's going to be n(l-1) because that's the number of units that
I'm feeding into each of the units in layer l. It turns out that if you're using a ReLu activation function that,
rather than 1 over n it turns out that, set in the variance of 2 over n
works a little bit better. So you often see that in initialization,
especially if you're using a ReLu activation function.
So if gl(z) is ReLu(z), oh and it depends on how familiar you are
with random variables. It turns out that something, a Gaussian random variable and then
multiplying it by a square root of this, that sets the variance to be quoted this way, to be 2 over n. And the reason I went from
n to this n superscript l-1 was, in this example with logistic regression which is at n input features, but the more general case layer l would have n(l-1) inputs
each of the units in that layer. So if the input features of activations are
roughly mean 0 and standard variance and variance 1 then this would cause z to also take on a similar scale. And this doesn't solve, but it definitely helps reduce the vanishing, exploding gradients problem,
because it's trying to set each of the weight matrices w, you know,
so that it's not too much bigger than 1 and not too much less than 1
so it doesn't explode or vanish too quickly. I've just mention some other variants. The version we just described is assuming a ReLu activation function
and this by a paper by her et al. A few other variants, if you are using a TanH activation function then there's a paper that shows that
instead of using the constant 2, it's better use the constant 1
and so 1 over this instead of 2. And so you multiply it by
the square root of this. So this square root term will replace this term and you use this
if you're using a TanH activation function. This is called Xavier initialization. And another version we're taught by
Yoshua Bengio and his colleagues, you might see in some papers, but is to use this formula, which you know has some other
theoretical justification, but I would say if you're using a
ReLu activation function, which is really the most common
activation function, I would use this formula. If you're using TanH you could try
this version instead, and some authors will also use this. But in practice I think all of these
formulas just give you a starting point. It gives you a default value to use
for the variance of the initialization of your weight matrices. If you wish the variance here, this variance parameter could be
another thing that you could tune with your hyperparameters.
So you could have another parameter that multiplies into
this formula and tune that multiplier as part of your
hyperparameter surge. Sometimes tuning the hyperparameter
has a modest size effect. It's not one of the first hyperparameters
I would usually try to tune, but I've also seen some
problems where tuning this helps a reasonable amount. But this is
usually lower down for me in terms of how important it is relative to the
other hyperparameters you can tune. So I hope that gives you some intuition
about the problem of vanishing or exploding gradients as well as choosing a reasonable scaling for
how you initialize the weights. Hopefully that makes your weights
not explode too quickly and not decay to zero too quickly,
so you can train a reasonably deep network without the weights or the gradients
exploding or vanishing too much. When you train deep networks,
this is another trick that will help you make your neural networks
trained much more quickly.
---


Lecture-12
---
When you implement back propagation
you'll find that there's a test called creating checking that can
really help you make sure that your implementation
of back prop is correct. Because sometimes you write all these
equations and you're just not 100% sure if you've got all the details right and
internal back propagation. So in order to build up to gradient and
checking, let's first talk about how to numerically
approximate computations of gradients and in the next video,
we'll talk about how you can implement gradient checking to make sure
the implementation of backdrop is correct. So let's take the function f and
replot it here and remember this is f of theta equals theta cubed, and let's
again start off to some value of theta. Let's say theta equals 1. Now instead of just nudging theta to
the right to get theta plus epsilon, we're going to nudge it to the right and nudge it to the left to get theta minus
epsilon, as was theta plus epsilon. So this is 1, this is 1.01,
this is 0.99 where, again, epsilon is the same as before, it is 0.01. It turns out that rather than
taking this little triangle and computing the height over the width,
you can get a much better estimate of the gradient if you take this point,
f of theta minus epsilon and this point, and you instead compute the height
over width of this bigger triangle. So for technical reasons which I won't go
into, the height over width of this bigger green triangle gives you a much better
approximation to the derivative at theta. And you saw it yourself, taking just
this lower triangle in the upper right is as if you have two triangles, right? This one on the upper right and
this one on the lower left. And you're kind of taking
both of them into account by using this bigger green triangle. So rather than a one sided difference,
you're taking a two sided difference. So let's work out the math. This point here is F
of theta plus epsilon. This point here is F of
theta minus epsilon. So the height of this big green
triangle is f of theta plus epsilon minus f of theta minus epsilon. And then the width,
this is 1 epsilon, this is 2 epsilon. So the width of this green
triangle is 2 epsilon. So the height of the width is
going to be first the height, so that's F of theta plus epsilon minus F of
theta minus epsilon divided by the width. So that was 2 epsilon which
we write that down here. And this should hopefully
be close to g of theta. So plug in the values,
remember f of theta is theta cubed. So this is theta plus epsilon is 1.01. So I take a cube of that minus 0.99 theta
cube of that divided by 2 times 0.01. Feel free to pause the video and
practice in the calculator. You should get that this is 3.0001. Whereas from the previous slide,
we saw that g of theta, this was 3 theta squared so
when theta was 1, so these two values are actually
very close to each other. The approximation error is now 0.0001. Whereas on the previous slide,
we've taken the one sided of difference just theta + theta +
epsilon we had gotten 3.0301 and so the approximation error
was 0.03 rather than 0.0001. So this two sided difference way of approximating the derivative you find
that this is extremely close to 3. And so this gives you a much greater
confidence that g of theta is probably a correct implementation
of the derivative of F. When you use this method for grading,
checking and back propagation, this turns out to run twice as slow as
you were to use a one-sided defense. It turns out that in practice I think it's
worth it to use this other method because it's just much more accurate. The little bit of optional theory for those of you that are a little bit more
familiar of Calculus, it turns out that, and it's okay if you don't get
what I'm about to say here. But it turns out that the formal
definition of a derivative is for very small values of epsilon is f of
theta plus epsilon minus f of theta minus epsilon over 2 epsilon. And the formal definition of
derivative is in the limits of exactly that formula on the right
as epsilon those as 0. And the definition of unlimited is
something that you learned if you took a Calculus class but
I won't go into that here. And it turns out that for
a non zero value of epsilon, you can show that the error of
this approximation is on the order of epsilon squared, and
remember epsilon is a very small number. So if epsilon is 0.01 which it is
here then epsilon squared is 0.0001. The big O notation means the error is
actually some constant times this, but this is actually exactly
our approximation error. So the big O constant happens to be 1. Whereas in contrast if we were to
use this formula, the other one, then the error is on the order of epsilon. And again, when epsilon is a number
less than 1, then epsilon is actually much bigger than epsilon squared which
is why this formula here is actually much less accurate approximation
than this formula on the left. Which is why when doing gradient checking,
we rather use this two-sided difference when you compute f of theta plus epsilon
minus f of theta minus epsilon and then divide by 2 epsilon rather than just one
sided difference which is less accurate. If you didn't understand my last two
comments, all of these things are on here. Don't worry about it. That's really more for those of you that
are a bit more familiar with Calculus, and with numerical approximations. But the takeaway is that this two-sided
difference formula is much more accurate. And so that's what we're going to use when
we do gradient checking in the next video. So you've seen how by taking
a two sided difference, you can numerically verify whether or
not a function g, g of theta that someone else gives you is a correct implementation
of the derivative of a function f. Let's now see how we can use
this to verify whether or not your back propagation
implementation is correct or if there might be a bug in there
that you need to go and tease out
---


Lecture-13
---
Gradient checking is a technique that's
helped me save tons of time, and helped me find bugs in my implementations
of back propagation many times. Let's see how you could
use it too to debug, or to verify that your implementation and
back process correct. So your new network will have some sort of
parameters, W1, B1 and so on up to WL bL. So to implement gradient checking, the
first thing you should do is take all your parameters and
reshape them into a giant vector data. So what you should do is take W which is
a matrix, and reshape it into a vector. You gotta take all of these Ws and reshape
them into vectors, and then concatenate all of these things, so
that you have a giant vector theta. Giant vector pronounced as theta. So we say that the cos function
J being a function of the Ws and Bs, You would now have the cost function
J being just a function of theta. Next, with W and B ordered the same way, you can also take dW[1], db[1] and
so on, and initiate them into big, giant vector d theta of
the same dimension as theta. So same as before, we shape dW[1] into
the matrix, db[1] is already a vector. We shape dW[L],
all of the dW's which are matrices. Remember, dW1 has
the same dimension as W1. db1 has the same dimension as b1. So the same sort of reshaping and
concatenation operation, you can then reshape all of these
derivatives into a giant vector d theta. Which has the same dimension as theta. So the question is, now,
is the theta the gradient or the slope of the cos function J? So here's how you implement
gradient checking, and often abbreviate gradient
checking to grad check. So first we remember that J Is now
a function of the giant parameter, theta, right? So expands to j is a function of theta 1,
theta 2, theta 3, and so on. Whatever's the dimension of this
giant parameter vector theta. So to implement grad check, what you're
going to do is implements a loop so that for each I, so for
each component of theta, let's compute D theta approx i to b. And let me take a two sided difference. So I'll take J of theta. Theta 1, theta 2, up to theta i. And we're going to nudge theta
i to add epsilon to this. So just increase theta i by epsilon,
and keep everything else the same. And because we're taking
a two sided difference, we're going to do the same on the other
side with theta i, but now minus epsilon. And then all of the other
elements of theta are left alone. And then we'll take this, and
we'll divide it by 2 theta. And what we saw from
the previous video is that this should be approximately
equal to d theta i. Of which is supposed to be the partial
derivative of J or of respect to, I guess theta i, if d theta i is
the derivative of the cost function J. So what you going to do is you're going to
compute to this for every value of i. And at the end,
you now end up with two vectors. You end up with this d theta approx, and this is going to be the same
dimension as d theta. And both of these are in turn
the same dimension as theta. And what you want to do is check if
these vectors are approximately equal to each other. So, in detail,
well how you do you define whether or not two vectors are really
reasonably close to each other? What I do is the following. I would compute the distance
between these two vectors, d theta approx minus d theta,
so just the o2 norm of this. Notice there's no square on top, so this is the sum of squares of
elements of the differences, and then you take a square root,
as you get the Euclidean distance. And then just to normalize by
the lengths of these vectors, divide by d theta approx plus d theta. Just take the Euclidean
lengths of these vectors. And the row for the denominator is just in
case any of these vectors are really small or really large, your the denominator
turns this formula into a ratio. So we implement this in practice, I use epsilon equals maybe 10
to the minus 7, so minus 7. And with this range of epsilon,
if you find that this formula gives you a value like 10 to the minus 7 or
smaller, then that's great. It means that your derivative
approximation is very likely correct. This is just a very small value. If it's maybe on the range of 10 to
the -5, I would take a careful look. Maybe this is okay. But I might double-check
the components of this vector, and make sure that none of
the components are too large. And if some of the components of
this difference are very large, then maybe you have a bug somewhere. And if this formula on the left is on
the other is -3, then I would wherever you have would be much more concerned
that maybe there's a bug somewhere. But you should really be getting
values much smaller then 10 minus 3. If any bigger than 10 to minus 3,
then I would be quite concerned. I would be seriously worried
that there might be a bug. And I would then,
you should then look at the individual components of data to see if
there's a specific value of i for which d theta across i is very
different from d theta i. And use that to try to
track down whether or not some of your derivative
computations might be incorrect. And after some amounts of debugging,
it finally, it ends up being this kind of very small value, then you
probably have a correct implementation. So when implementing a neural network, what often happens is I'll implement
foreprop, implement backprop. And then I might find that this grad
check has a relatively big value. And then I will suspect that there must
be a bug, go in debug, debug, debug. And after debugging for a while, If I find
that it passes grad check with a small value, then you can be much more
confident that it's then correct. So you now know how
gradient checking works. This has helped me find lots of bugs
in my implementations of neural nets, and I hope it'll help you too. In the next video,
I want to share with you some tips or some notes on how to actually
implement gradient checking. Let's go onto the next video.
---


Lecture-14
---
In the last video you learned
about gradient checking. In this video, I want to share
with you some practical tips or some notes on how to actually go about
implementing this for your neural network. First, don't use grad check in training,
only to debug. So what I mean is that,
computing d theta approx i, for all the values of i,
this is a very slow computation. So to implement gradient descent,
you'd use backprop to compute d theta and just use backprop to
compute the derivative. And it's only when you're debugging
that you would compute this to make sure it's close to d theta. But once you've done that, then you
would turn off the grad check, and don't run this during every
iteration of gradient descent, because that's just much too slow. Second, if an algorithm fails grad check,
look at the components, look at the individual components,
and try to identify the bug. So what I mean by that is if d theta
approx is very far from d theta, what I would do is look at the different
values of i to see which are the values of d theta approx that are really very
different than the values of d theta. So for example, if you find that
the values of theta or d theta, they're very far off, all correspond
to dbl for some layer or for some layers, but the components for
dw are quite close, right? Remember, different components of theta
correspond to different components of b and w. When you find this is the case,
then maybe you find that the bug is in how you're computing db, the derivative
with respect to parameters b. And similarly, vice versa, if you find
that the values that are very far, the values from d theta approx
that are very far from d theta, you find all those components came
from dw or from dw in a certain layer, then that might help you hone
in on the location of the bug. This doesn't always let you
identify the bug right away, but sometimes it helps you give you some
guesses about where to track down the bug. Next, when doing grad check, remember your regularization term
if you're using regularization. So if your cost function is J of
theta equals 1 over m sum of your losses and
then plus this regularization term. And sum over l of wl squared,
then this is the definition of J. And you should have that d
theta is gradient of J with respect to theta,
including this regularization term. So just remember to include that term. Next, grad check doesn't work with
dropout, because in every iteration, dropout is randomly eliminating
different subsets of the hidden units. There isn't an easy to compute
cost function J that dropout is doing gradient descent on. It turns out that dropout can be viewed
as optimizing some cost function J, but it's cost function J defined by
summing over all exponentially large subsets of nodes they could
eliminate in any iteration. So the cost function J is very
difficult to compute, and you're just sampling the cost function every time you eliminate different
random subsets in those we use dropout. So it's difficult to use grad
check to double check your computation with dropouts. So what I usually do is implement
grad check without dropout. So if you want, you can set keep-prob and
dropout to be equal to 1.0. And then turn on dropout and hope that my
implementation of dropout was correct. There are some other things you could do,
like fix the pattern of nodes dropped and verify that grad check for that
pattern of [INAUDIBLE] is correct, but in practice I don't usually do that. So my recommendation is turn off dropout,
use grad check to double check that your algorithm is at least correct without
dropout, and then turn on dropout. Finally, this is a subtlety. It is not impossible, rarely happens,
but it's not impossible that your implementation of gradient descent is
correct when w and b are close to 0, so at random initialization. But that as you run gradient descent and
w and b become bigger, maybe your implementation of backprop is
correct only when w and b is close to 0, but it gets more inaccurate when w and
b become large. So one thing you could do,
I don't do this very often, but one thing you could do is run grad
check at random initialization and then train the network for
a while so that w and b have some time to wander away from 0,
from your small random initial values. And then run grad check again after you've
trained for some number of iterations. So that's it for gradient checking. And congratulations for coming to
the end of this week's materials. In this week, you've learned about how to
set up your train, dev, and test sets, how to analyze bias and variance and what
things to do if you have high bias versus high variance versus maybe high bias and
high variance. You also saw how to apply
different forms of regularization, like L2 regularization and
dropout on your neural network. So some tricks for speeding up
the training of your neural network. And then finally, gradient checking. So I think you've seen
a lot in this week and you get to exercise a lot of these ideas
in this week's programming exercise. So best of luck with that, and I look forward to seeing you
in the week two materials.
---



